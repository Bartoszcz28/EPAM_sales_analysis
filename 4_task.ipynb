{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb8d926",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "**Top 2 Highest earning sellers by each location.**</br>\n",
    "> We have 2 tables with sales history and data with all of the sellers. Sellers are assigned to one location. Our task is to find two sellers with highest profits grouped by location. Because we have information about state and city we will find top sellers grouping by those two categories. We assume that the revenue is equal to price of product subtracted by the freight price that the sellers has to settle. At the end we will create the map of the whole country with scaled markers according to the number of sales done and revenue gained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fba95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|3442f8959a84dea7e...|                 13023|   campinas|          SP|\n",
      "|d1b65fc7debc3361e...|                 13844| mogi guacu|          SP|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Orders data\n",
    "# \"order_id\",\"order_item_id\",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"price\",\"freight_value\"\n",
    "olist_order_items_dataset = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_order_items_dataset.csv\")\n",
    "olist_order_items_dataset.show(2)\n",
    "# Sellers data \n",
    "# \"seller_id\",\"seller_zip_code_prefix\",\"seller_city\",\"seller_state\"\n",
    "olist_sellers_dataset = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_sellers_dataset.csv\")\n",
    "olist_sellers_dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278fac42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+--------+\n",
      "|           seller_id|seller_state|   seller_city|   sales|\n",
      "+--------------------+------------+--------------+--------+\n",
      "|7142540dd4c91e223...|          SP|     penapolis|37373.56|\n",
      "|897060da8b9a21f65...|          SP|ribeirao preto|23023.92|\n",
      "|318f287a62ab7ac10...|          SP|     sao paulo| 2517.48|\n",
      "|609e1a9a6c2539919...|          SC|       brusque| 6595.23|\n",
      "+--------------------+------------+--------------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining sales with sellers \n",
    "# And calculating sum of all the sales \n",
    "items = olist_order_items_dataset.select(\"seller_id\",\"price\")\n",
    "sellers = olist_sellers_dataset.select(\"seller_id\",\"seller_city\",\"seller_state\")\n",
    "sales_df = items.join(sellers, ['seller_id'], \"Inner\")\n",
    "sales_df = sales_df\\\n",
    "    .groupBy('seller_id','seller_state','seller_city')\\\n",
    "    .agg({'price':'sum'})\\\n",
    "    .select('seller_id','seller_state','seller_city',round('sum(price)',2))\\\n",
    "    .withColumnRenamed(\"round(sum(price), 2)\", \"sales\")\n",
    "sales_df.show(4)\n",
    "sales_df.createOrReplaceTempView(\"join_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d009f29",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every state\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14138b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Partition by state\n",
    "window_state = Window.partitionBy('seller_state').orderBy(col('sales').desc())\n",
    "state_df = sales_df\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_state','sales','rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e43c9a",
   "metadata": {},
   "source": [
    "**using pyspark sql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ebace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"WITH seller AS (\n",
    "               SELECT\n",
    "               seller_id,\n",
    "               seller_state,\n",
    "               ROUND(SUM(sales)) AS sum_price\n",
    "               FROM join_table\n",
    "               GROUP BY seller_state, seller_id\n",
    "               )\n",
    "               SELECT seller_id, seller_state, sum_price\n",
    "               FROM (SELECT *,\n",
    "               ROW_NUMBER() OVER(PARTITION BY seller_state ORDER BY sum_price DESC) AS row_number\n",
    "               FROM seller)\n",
    "               WHERE row_number < 3\n",
    "               \"\"\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15a8ff",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every city\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52765bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by city\n",
    "window_state = Window.partitionBy('seller_city').orderBy(col('sales').desc())\n",
    "state_df = sales_df\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_city','sales','rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6171b65",
   "metadata": {},
   "source": [
    "**using pyspark sql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028a41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"WITH seller AS (\n",
    "               SELECT\n",
    "               seller_id,\n",
    "               seller_city,\n",
    "               ROUND(SUM(sales)) AS sum_price\n",
    "               FROM join_table\n",
    "               GROUP BY seller_city, seller_id\n",
    "               )\n",
    "               SELECT seller_id, seller_city, sum_price\n",
    "               FROM (SELECT *,\n",
    "               ROW_NUMBER() OVER(PARTITION BY seller_city ORDER BY sum_price DESC) AS row_number\n",
    "               FROM seller)\n",
    "               WHERE row_number < 3\n",
    "               \"\"\")\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09198c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+----+\n",
      "|           seller_id|seller_city|   sales|rank|\n",
      "+--------------------+-----------+--------+----+\n",
      "|da20530872245d6cd...|  igrejinha|  314.96|   1|\n",
      "|c33847515fa6305ce...|    brusque|15519.85|   1|\n",
      "|ad97a199236354e53...|    brusque| 13205.9|   2|\n",
      "|2c4c47cb51acd5ea5...|   buritama|  2575.9|   1|\n",
      "|f181738b150df1f37...|carapicuiba|  5529.7|   1|\n",
      "|f680f85bee2d25355...|carapicuiba| 5183.92|   2|\n",
      "+--------------------+-----------+--------+----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------+----+\n",
      "|           seller_id|seller_city|   sales|rank|\n",
      "+--------------------+-----------+--------+----+\n",
      "|da20530872245d6cd...|  igrejinha|  314.96|   1|\n",
      "|c33847515fa6305ce...|    brusque|15519.85|   1|\n",
      "|ad97a199236354e53...|    brusque| 13205.9|   2|\n",
      "|2c4c47cb51acd5ea5...|   buritama|  2575.9|   1|\n",
      "|f181738b150df1f37...|carapicuiba|  5529.7|   1|\n",
      "|f680f85bee2d25355...|carapicuiba| 5183.92|   2|\n",
      "+--------------------+-----------+--------+----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 6 results of final answer for testing\n",
    "# Top sellers by states \n",
    "state_df.show(6)\n",
    "\n",
    "# Show 6 results of final answer for testing\n",
    "# Top sellers by city \n",
    "state_df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81832c5",
   "metadata": {},
   "source": [
    "### Creating map with sales markers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca929af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting city names with geo codes \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Reading geolocation data and creating one geo coords per city \n",
    "geo = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_geolocation_dataset.csv\").select('geolocation_city','geolocation_lng','geolocation_lat')\n",
    "geo = geo.groupBy('geolocation_city')\\\n",
    "    .agg({'geolocation_lat':'avg','geolocation_lng':'avg'})\\\n",
    "    .orderBy('geolocation_city')\\\n",
    "    .withColumnRenamed('avg(geolocation_lat)','lat')\\\n",
    "    .withColumnRenamed('avg(geolocation_lng)','lng')\\\n",
    "    .withColumnRenamed('geolocation_city','seller_city')\n",
    "\n",
    "# Joining sales df and location df \n",
    "loc_df = sales_df\\\n",
    "    .groupBy('seller_city')\\\n",
    "    .agg({'sales':'count'})\\\n",
    "    .withColumnRenamed('count(sales)','num_sales')\\\n",
    "    .join(geo,['seller_city'])\n",
    "# Adding fraction of all of the sales for point scaling \n",
    "sum_sales = loc_df.agg({'num_sales':'sum'}).collect()[0][0]\n",
    "loc_df = loc_df\\\n",
    "    .withColumn('fract',col('num_sales')/sum_sales)\\\n",
    "    .orderBy(col('fract'))\\\n",
    "    .select('seller_city','lat','lng','fract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac798ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "+----------------+---------+-------------------+-------------------+\n",
    "|     seller_city|num_sales|                lat|                lng|\n",
    "+----------------+---------+-------------------+-------------------+\n",
    "|       igrejinha|        1|-29.571977530057968|-50.794729763963296|\n",
    "|         brusque|       10|-27.100857082718772| -48.91495449463352|\n",
    "|        buritama|        1| -21.06728750877154| -50.14440187114215|\n",
    "|     carapicuiba|       10|-23.545578557335205| -46.83877557576887|\n",
    "|fernando prestes|        1|-21.267134804144067|-48.686678524301236|\n",
    "|           garca|        4|-22.211232140083208| -49.65819174411557|\n",
    "+----------------+---------+-------------------+-------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize folium map\n",
    "sales_map = folium.Map(\n",
    "    zoom_start=4,\n",
    "    location=[-23.54, -48.91], prefer_canvas=True)\n",
    "def group_val(x):\n",
    "    if x>=0.2:\n",
    "        return 0\n",
    "    elif x>=0.02:\n",
    "        return 1\n",
    "    elif x>=0.008:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f295bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values from loc_df\n",
    "# loc_data[i][j] | i for row num and j for 0-city, 1-lat, 2-lng, 3-fract\n",
    "LAT = 1\n",
    "LNG = 2\n",
    "FRA = 3\n",
    "colors = ['red','orange','yellow','green']\n",
    "sizes = [13,8,2,1]\n",
    "# Collect max num of sales \n",
    "loc_data = loc_df.collect()\n",
    "i = 0\n",
    "# W poprzednim pliku pętla for dla pysparka była spowolniona \n",
    "# przez to że collect był wywoływany w każdej iteracji pętli \n",
    "# zamiast raz przed pętlą for\n",
    "for row in range(loc_df.count()):\n",
    "    folium.CircleMarker(\n",
    "        location=[loc_data[i][LAT],loc_data[i][LNG]],\n",
    "        radius=sizes[group_val(loc_data[i][FRA])],\n",
    "        color=colors[group_val(loc_data[i][FRA])],\n",
    "        fill=True,\n",
    "        fill_color=colors[group_val(loc_data[i][FRA])],\n",
    "        fill_opacity=1\n",
    "    ).add_to(sales_map)\n",
    "    i += 1\n",
    "sales_map.save('sales_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860f797",
   "metadata": {},
   "source": [
    "# Map with markers scaled by number of sales done in the given city\n",
    "<img src='sales_map.png'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
