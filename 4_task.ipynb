{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb8d926",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "**Top 2 Highest earning sellers by each location.**</br>\n",
    "> We have 2 tables with sales history and data with all of the sellers. Sellers are assigned to one location. Our task is to find two sellers with highest profits grouped by location. Because we have information about state and city we will find top sellers grouping by those two categories. We assume that the revenue is equal to price of a product. At the end we will create the map of the whole country with scaled markers according to the number of sales done and revenue gained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b572d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fba95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from parquet and selecting only needed columns\n",
    "# Orders data\n",
    "items = spark.read.parquet('data_parquet/olist_order_items_dataset.parquet')\\\n",
    "    .select('order_id','seller_id','price')\n",
    "# Sellers data \n",
    "sellers = spark.read.parquet('data_parquet/olist_sellers_dataset.parquet')\\\n",
    "    .select('seller_id','seller_zip_code_prefix','seller_city','seller_state')\\\n",
    "    .withColumnRenamed('seller_zip_code_prefix','zip_code')\n",
    "# Geolocation data \n",
    "geo = spark.read.parquet('data_parquet/olist_geolocation_dataset.parquet')\\\n",
    "    .select('geolocation_zip_code_prefix','geolocation_lat','geolocation_lng')\\\n",
    "    .withColumnRenamed('geolocation_zip_code_prefix','zip_code')\n",
    "# Orders data \n",
    "orders = spark.read.parquet('data_parquet/olist_orders_dataset.parquet')\\\n",
    "    .select('order_id','order_purchase_timestamp')\n",
    "\n",
    "# |-- order_id: string (nullable = true)\n",
    "# |-- seller_id: string (nullable = true)\n",
    "# |-- price: double (nullable = true)\n",
    "\n",
    "# |-- seller_id: string (nullable = true)\n",
    "# |-- seller_zip_code_prefix: integer (nullable = true)\n",
    "# |-- seller_city: string (nullable = true)\n",
    "# |-- seller_state: string (nullable = true)\n",
    "\n",
    "# |-- geolocation_zip_code_prefix: integer (nullable = true)\n",
    "# |-- geolocation_lat: double (nullable = true)\n",
    "# |-- geolocation_lng: double (nullable = true)\n",
    "\n",
    "# |-- order_id: string (nullable = true)\n",
    "# |-- order_purchase_timestamp: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278fac42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating sum of money earned by sellers \n",
    "# and joining sellers table to be able to calculate \n",
    "# money earned partitioning by location \n",
    "sales = items\\\n",
    "    .groupBy('seller_id')\\\n",
    "    .agg({'price':'sum'})\\\n",
    "    .withColumnRenamed('sum(price)','revenue')\\\n",
    "    .join(sellers,['seller_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d009f29",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every state\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14138b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Partition by state\n",
    "window_state = Window.partitionBy('seller_state').orderBy(col('revenue').desc())\n",
    "state_df = sales\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_state','revenue')\\\n",
    "    .orderBy(col('revenue').desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15a8ff",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every city\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52765bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by city\n",
    "window_state = Window.partitionBy('seller_city').orderBy(col('revenue').desc())\n",
    "city_df = sales\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_city','revenue')\\\n",
    "    .orderBy(col('revenue').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09198c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 sellers from every state:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+\n",
      "|           seller_id|seller_state|           revenue|\n",
      "+--------------------+------------+------------------+\n",
      "|4869f7a5dfa277a7d...|          SP| 229472.6300000005|\n",
      "|53243585a1d6dc264...|          BA|222776.05000000002|\n",
      "|4a3ca9315b744ce9f...|          SP| 200472.9200000013|\n",
      "|46dc3b2cc0980fb8e...|          RJ|128111.19000000028|\n",
      "|620c87c171fb2a6dd...|          RJ|114774.50000000041|\n",
      "|a1043bafd471dff53...|          MG|101901.16000000018|\n",
      "|ccc4bbb5f32a6ab2b...|          PR|          74004.62|\n",
      "|04308b1ee57b6625f...|          SC| 60130.59999999999|\n",
      "|522620dcb18a6b31c...|          PR| 57168.48999999999|\n",
      "|de722cd6dad950a92...|          PE|55426.099999999926|\n",
      "|25c5c91f63607446a...|          MG| 54679.21999999999|\n",
      "|eeb6de78f79159600...|          SC|43739.840000000004|\n",
      "+--------------------+------------+------------------+\n",
      "only showing top 12 rows\n",
      "\n",
      "\n",
      "Top 2 sellers from every city:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 81:============================================>         (163 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------------+\n",
      "|           seller_id|     seller_city|           revenue|\n",
      "+--------------------+----------------+------------------+\n",
      "|4869f7a5dfa277a7d...|         guariba| 229472.6300000005|\n",
      "|53243585a1d6dc264...|lauro de freitas|222776.05000000002|\n",
      "|4a3ca9315b744ce9f...|        ibitinga| 200472.9200000013|\n",
      "|fa1c13f2614d7b5c4...|          sumare|194042.03000000038|\n",
      "|7c67e1448b00f6e96...| itaquaquecetuba|         187923.89|\n",
      "|7e93a43ef30c4f03f...|         barueri|         176431.87|\n",
      "|da8622b14eb17ae28...|      piracicaba|160236.57000000114|\n",
      "|7a67c85e85bb2ce85...|       sao paulo|141745.53000000032|\n",
      "|1025f0e2d44d7041d...|       sao paulo|138968.55000000022|\n",
      "|46dc3b2cc0980fb8e...|  rio de janeiro|128111.19000000028|\n",
      "|620c87c171fb2a6dd...|      petropolis|114774.50000000041|\n",
      "|7d13fca1522535862...|  ribeirao preto|113628.97000000007|\n",
      "+--------------------+----------------+------------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 82:===============================================>      (177 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Showing first 12 results from answer. First calculated for states and second for cities. \n",
    "print('Top 2 sellers from every state:\\n')\n",
    "state_df.show(12)\n",
    "print('\\nTop 2 sellers from every city:\\n')\n",
    "city_df.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81832c5",
   "metadata": {},
   "source": [
    "### Creating map with sales markers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca929af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting city names with geo codes \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Reading geolocation data and creating one geo coords per city \n",
    "geo = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_geolocation_dataset.csv\").select('geolocation_city','geolocation_lng','geolocation_lat')\n",
    "geo = geo.groupBy('geolocation_city')\\\n",
    "    .agg({'geolocation_lat':'avg','geolocation_lng':'avg'})\\\n",
    "    .orderBy('geolocation_city')\\\n",
    "    .withColumnRenamed('avg(geolocation_lat)','lat')\\\n",
    "    .withColumnRenamed('avg(geolocation_lng)','lng')\\\n",
    "    .withColumnRenamed('geolocation_city','seller_city')\n",
    "\n",
    "# Joining sales df and location df \n",
    "loc_df = sales_df\\\n",
    "    .groupBy('seller_city')\\\n",
    "    .agg({'sales':'count'})\\\n",
    "    .withColumnRenamed('count(sales)','num_sales')\\\n",
    "    .join(geo,['seller_city'])\n",
    "# Adding fraction of all of the sales for point scaling \n",
    "sum_sales = loc_df.agg({'num_sales':'sum'}).collect()[0][0]\n",
    "loc_df = loc_df\\\n",
    "    .withColumn('fract',col('num_sales')/sum_sales)\\\n",
    "    .orderBy(col('fract'))\\\n",
    "    .select('seller_city','lat','lng','fract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac798ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''+----------------+---------+-------------------+-------------------+\n",
    "|     seller_city|num_sales|                lat|                lng|\n",
    "+----------------+---------+-------------------+-------------------+\n",
    "|       igrejinha|        1|-29.571977530057968|-50.794729763963296|\n",
    "|         brusque|       10|-27.100857082718772| -48.91495449463352|\n",
    "|        buritama|        1| -21.06728750877154| -50.14440187114215|\n",
    "|     carapicuiba|       10|-23.545578557335205| -46.83877557576887|\n",
    "|fernando prestes|        1|-21.267134804144067|-48.686678524301236|\n",
    "|           garca|        4|-22.211232140083208| -49.65819174411557|\n",
    "+----------------+---------+-------------------+-------------------+'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize folium map\n",
    "sales_map = folium.Map(\n",
    "    zoom_start=4,\n",
    "    location=[-23.54, -48.91], prefer_canvas=True)\n",
    "def group_val(x):\n",
    "    if x>=0.2:\n",
    "        return 0\n",
    "    elif x>=0.02:\n",
    "        return 1\n",
    "    elif x>=0.008:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f295bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values from loc_df\n",
    "# loc_data[i][j] | i for row num and j for 0-city, 1-lat, 2-lng, 3-fract\n",
    "LAT = 1\n",
    "LNG = 2\n",
    "FRA = 3\n",
    "colors = ['red','orange','yellow','green']\n",
    "sizes = [13,8,2,1]\n",
    "# Collect max num of sales \n",
    "loc_data = loc_df.collect()\n",
    "i = 0\n",
    "# W poprzednim pliku pętla for dla pysparka była spowolniona \n",
    "# przez to że collect był wywoływany w każdej iteracji pętli \n",
    "# zamiast raz przed pętlą for\n",
    "for row in range(loc_df.count()):\n",
    "    folium.CircleMarker(\n",
    "        location=[loc_data[i][LAT],loc_data[i][LNG]],\n",
    "        radius=sizes[group_val(loc_data[i][FRA])],\n",
    "        color=colors[group_val(loc_data[i][FRA])],\n",
    "        fill=True,\n",
    "        fill_color=colors[group_val(loc_data[i][FRA])],\n",
    "        fill_opacity=1,\n",
    "        popup=loc_data[i][FRA],\n",
    "        tooltip=loc_data[i][FRA]\n",
    "    ).add_to(sales_map)\n",
    "    i += 1\n",
    "sales_map.save('sales_map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860f797",
   "metadata": {},
   "source": [
    "# Map with markers scaled by number of sales done in the given city\n",
    "<img src='sales_map.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce10311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pillow\n",
    "#!pip install selenium\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "img_data = sales_map._to_png(5)\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "img.save('sales_map.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
