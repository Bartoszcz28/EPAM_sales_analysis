{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaf1d55",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "**Top 2 Highest earning sellers by each location.**</br>\n",
    "> We have 2 tables with sales history and data with all of the sellers. Sellers are assigned to one location. Our task is to find two sellers with highest profits grouped by location. Because we have information about state and city we will find top sellers grouping by those two categories. We assume that the revenue is equal to price of product subtracted by the freight price that the sellers has to settle. At the end we will create the map of the whole country with scaled markers according to the number of sales done and revenue gained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b572d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fba95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|3442f8959a84dea7e...|                 13023|   campinas|          SP|\n",
      "|d1b65fc7debc3361e...|                 13844| mogi guacu|          SP|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Orders data\n",
    "# \"order_id\",\"order_item_id\",\"product_id\",\"seller_id\",\"shipping_limit_date\",\"price\",\"freight_value\"\n",
    "olist_order_items_dataset = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_order_items_dataset.csv\")\n",
    "olist_order_items_dataset.show(2)\n",
    "# Sellers data \n",
    "# \"seller_id\",\"seller_zip_code_prefix\",\"seller_city\",\"seller_state\"\n",
    "olist_sellers_dataset = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "                            .csv(\"data/olist_sellers_dataset.csv\")\n",
    "olist_sellers_dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "278fac42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+--------+\n",
      "|           seller_id|seller_state|   seller_city|   sales|\n",
      "+--------------------+------------+--------------+--------+\n",
      "|7142540dd4c91e223...|          SP|     penapolis|37373.56|\n",
      "|897060da8b9a21f65...|          SP|ribeirao preto|23023.92|\n",
      "|318f287a62ab7ac10...|          SP|     sao paulo| 2517.48|\n",
      "|609e1a9a6c2539919...|          SC|       brusque| 6595.23|\n",
      "+--------------------+------------+--------------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining sales with sellers \n",
    "# And calculating sum of all the sales \n",
    "items = olist_order_items_dataset.select(\"seller_id\",\"price\")\n",
    "sellers = olist_sellers_dataset.select(\"seller_id\",\"seller_city\",\"seller_state\")\n",
    "sales_df = items.join(sellers, ['seller_id'], \"Inner\")\n",
    "sales_df = sales_df\\\n",
    "    .groupBy('seller_id','seller_state','seller_city')\\\n",
    "    .agg({'price':'sum'})\\\n",
    "    .select('seller_id','seller_state','seller_city',round('sum(price)',2))\\\n",
    "    .withColumnRenamed(\"round(sum(price), 2)\", \"sales\")\n",
    "sales_df.show(4)\n",
    "sales_df.createOrReplaceTempView(\"join_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd87d03",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every state\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eef4b0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+----+\n",
      "|           seller_id|seller_state|   sales|rank|\n",
      "+--------------------+------------+--------+----+\n",
      "|04308b1ee57b6625f...|          SC| 60130.6|   1|\n",
      "|eeb6de78f79159600...|          SC|43739.84|   2|\n",
      "|3364a91ec4d56c98e...|          RO| 3579.94|   1|\n",
      "|a5259c149128e82c9...|          RO| 1182.26|   2|\n",
      "|47efca563408aae19...|          PI|  2522.0|   1|\n",
      "|327b89b872c14d1c0...|          AM|  1177.0|   1|\n",
      "+--------------------+------------+--------+----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partition by state\n",
    "window_state = Window.partitionBy('seller_state').orderBy(col('sales').desc())\n",
    "state_df = sales_df\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_state','sales','rank')\n",
    "# Show 10 results for testing\n",
    "state_df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b828776",
   "metadata": {},
   "source": [
    "**using pyspark sql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a0d26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 224:==============================================>      (177 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------+\n",
      "|           seller_id|seller_state|sum_price|\n",
      "+--------------------+------------+---------+\n",
      "|04308b1ee57b6625f...|          SC|  60131.0|\n",
      "|eeb6de78f79159600...|          SC|  43740.0|\n",
      "|3364a91ec4d56c98e...|          RO|   3580.0|\n",
      "|a5259c149128e82c9...|          RO|   1182.0|\n",
      "+--------------------+------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"WITH seller AS (\n",
    "               SELECT\n",
    "               seller_id,\n",
    "               seller_state,\n",
    "               ROUND(SUM(sales)) AS sum_price\n",
    "               FROM join_table\n",
    "               GROUP BY seller_state, seller_id\n",
    "               )\n",
    "               SELECT seller_id, seller_state, sum_price\n",
    "               FROM (SELECT *,\n",
    "               ROW_NUMBER() OVER(PARTITION BY seller_state ORDER BY sum_price DESC) AS row_number\n",
    "               FROM seller)\n",
    "               WHERE row_number < 3\n",
    "               \"\"\")\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da589b9",
   "metadata": {},
   "source": [
    "### Top 2 sellers in every city\n",
    "**using pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f89b2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 132:=====================================>               (142 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------+----+\n",
      "|           seller_id|     seller_city|   sales|rank|\n",
      "+--------------------+----------------+--------+----+\n",
      "|da20530872245d6cd...|       igrejinha|  314.96|   1|\n",
      "|c33847515fa6305ce...|         brusque|15519.85|   1|\n",
      "|ad97a199236354e53...|         brusque| 13205.9|   2|\n",
      "|2c4c47cb51acd5ea5...|        buritama|  2575.9|   1|\n",
      "|f181738b150df1f37...|     carapicuiba|  5529.7|   1|\n",
      "|f680f85bee2d25355...|     carapicuiba| 5183.92|   2|\n",
      "|60da8bfa7eebe230b...|fernando prestes|    86.6|   1|\n",
      "|527801b552d0077ff...|           garca| 17942.8|   1|\n",
      "|c12b92bf1c350f3e6...|           garca| 4808.78|   2|\n",
      "|e333046ce6517bd8b...|         ipaussu|  7268.0|   1|\n",
      "+--------------------+----------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partition by city\n",
    "window_state = Window.partitionBy('seller_city').orderBy(col('sales').desc())\n",
    "state_df = sales_df\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_city','sales','rank')\n",
    "# Show 10 results for testing\n",
    "state_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6171b65",
   "metadata": {},
   "source": [
    "**using pyspark sql**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9028a41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 250:==============================================>      (175 + 9) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+\n",
      "|           seller_id|seller_city|sum_price|\n",
      "+--------------------+-----------+---------+\n",
      "|da20530872245d6cd...|  igrejinha|    315.0|\n",
      "|c33847515fa6305ce...|    brusque|  15520.0|\n",
      "|ad97a199236354e53...|    brusque|  13206.0|\n",
      "|2c4c47cb51acd5ea5...|   buritama|   2576.0|\n",
      "|f181738b150df1f37...|carapicuiba|   5530.0|\n",
      "|f680f85bee2d25355...|carapicuiba|   5184.0|\n",
      "+--------------------+-----------+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 251:==================================================>  (190 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"WITH seller AS (\n",
    "               SELECT\n",
    "               seller_id,\n",
    "               seller_city,\n",
    "               ROUND(SUM(sales)) AS sum_price\n",
    "               FROM join_table\n",
    "               GROUP BY seller_city, seller_id\n",
    "               )\n",
    "               SELECT seller_id, seller_city, sum_price\n",
    "               FROM (SELECT *,\n",
    "               ROW_NUMBER() OVER(PARTITION BY seller_city ORDER BY sum_price DESC) AS row_number\n",
    "               FROM seller)\n",
    "               WHERE row_number < 3\n",
    "               \"\"\")\n",
    "df.show(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
