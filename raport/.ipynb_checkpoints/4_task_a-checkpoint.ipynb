{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d48cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/19 13:50:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/19 13:50:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|seller_state|       1_best_seller|       2_best_seller|\n",
      "+------------+--------------------+--------------------+\n",
      "|          SC|04308b1ee57b6625f...|eeb6de78f79159600...|\n",
      "|          RO|3364a91ec4d56c98e...|a5259c149128e82c9...|\n",
      "|          PI|47efca563408aae19...|In this state is ...|\n",
      "|          AM|327b89b872c14d1c0...|In this state is ...|\n",
      "|          GO|9d5a9018aee56acb3...|750303a20e9c56b2a...|\n",
      "|          MT|2dee2ce60de9709b1...|abcd2cb37d46c2c8f...|\n",
      "|          SP|4869f7a5dfa277a7d...|4a3ca9315b744ce9f...|\n",
      "|          ES|001cca7ae9ae17fb1...|33dd941c27854f762...|\n",
      "|          PB|a6bd7d1ccdac48c6b...|07017df32dc5f2f1d...|\n",
      "|          RS|87142160b41353c4e...|b32be1695eb7ec5f1...|\n",
      "|          MS|b1fecf4da1fa2689b...|9c068d10aca38e85c...|\n",
      "|          MG|a1043bafd471dff53...|25c5c91f63607446a...|\n",
      "|          PA|67225bff54a172ff6...|In this state is ...|\n",
      "|          BA|53243585a1d6dc264...|c72de06d72748d1a0...|\n",
      "|          SE|4b39558c138930b9e...|c53bcd3be457a342a...|\n",
      "|          PE|de722cd6dad950a92...|40db9e9aa57f7bb15...|\n",
      "|          CE|bbf9ad41dca6603e6...|dbdd0ec73a4817971...|\n",
      "|          RN|02d35243ea2e49733...|52562a9f449c3dc3d...|\n",
      "|          RJ|46dc3b2cc0980fb8e...|620c87c171fb2a6dd...|\n",
      "|          MA|06a2c3af7b3aee5d6...|In this state is ...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/19 13:51:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         seller_city|       1_best_seller|       2_best_seller|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|           igrejinha|da20530872245d6cd...|In this city is o...|\n",
      "|             brusque|c33847515fa6305ce...|ad97a199236354e53...|\n",
      "|            buritama|2c4c47cb51acd5ea5...|In this city is o...|\n",
      "|         carapicuiba|f181738b150df1f37...|f680f85bee2d25355...|\n",
      "|    fernando prestes|60da8bfa7eebe230b...|In this city is o...|\n",
      "|               garca|527801b552d0077ff...|c12b92bf1c350f3e6...|\n",
      "|             ipaussu|e333046ce6517bd8b...|In this city is o...|\n",
      "|  sao joao de meriti|3c03b12bab54d8b37...|117cfc326c6d50da6...|\n",
      "|              araras|45213867cefbf2cd4...|31da954dc0855f249...|\n",
      "|           jacutinga|7a241947449cc45db...|198c7ea11960a9844...|\n",
      "|       nova friburgo|2bf6a2c1e71bbd29a...|c26a2be5b53b7db6b...|\n",
      "| sao pedro da aldeia|da6a60cc8cc724fe5...|In this city is o...|\n",
      "|itapecerica da serra|c9c7905cffc4ef9ff...|3f995f07c49d0d55a...|\n",
      "|              santos|1c5e4e49b90794802...|36a968b544695394e...|\n",
      "|            ibitinga|4a3ca9315b744ce9f...|cca3071e3e9bb7d12...|\n",
      "|               muqui|15aac934c58d88678...|In this city is o...|\n",
      "|              cuiaba|abcd2cb37d46c2c8f...|99002261c568a84cc...|\n",
      "|     franco da rocha|70bf57d811208f892...|6561d6bf844e464b4...|\n",
      "|              centro|d5c530f4884a75ae0...|In this city is o...|\n",
      "|               cotia|d650b663c3b5f6fb3...|33ac3e28642ab8bda...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Reading data from parquet and selecting only needed columns\n",
    "# Orders data\n",
    "items = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv('/home/jovyan/work/EPAM_sales_analysis/data/olist_order_items_dataset.csv')\\\n",
    "    .select('order_id','seller_id','price')\n",
    "\n",
    "# Sellers data \n",
    "sellers = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv('/home/jovyan/work/EPAM_sales_analysis/data/olist_sellers_dataset.csv')\\\n",
    "    .select('seller_id','seller_zip_code_prefix','seller_city','seller_state')\\\n",
    "    .withColumnRenamed('seller_zip_code_prefix','zip_code')\n",
    "\n",
    "# Geolocation data \n",
    "geo = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv('/home/jovyan/work/EPAM_sales_analysis/data/olist_geolocation_dataset.csv')\\\n",
    "    .select('geolocation_zip_code_prefix','geolocation_lat','geolocation_lng')\\\n",
    "    .withColumnRenamed('geolocation_zip_code_prefix','zip_code')\\\n",
    "    .groupBy('zip_code')\\\n",
    "    .agg({'geolocation_lat':'avg','geolocation_lng':'avg'})\\\n",
    "    .withColumnRenamed('avg(geolocation_lat)','lat')\\\n",
    "    .withColumnRenamed('avg(geolocation_lng)','lng')\n",
    "\n",
    "# Orders data \n",
    "orders = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "    .csv('/home/jovyan/work/EPAM_sales_analysis/data/olist_orders_dataset.csv')\\\n",
    "    .select('order_id','order_purchase_timestamp')\n",
    "\n",
    "# |-- order_id: string (nullable = true)\n",
    "# |-- seller_id: string (nullable = true)\n",
    "# |-- price: double (nullable = true)\n",
    "\n",
    "# |-- seller_id: string (nullable = true)\n",
    "# |-- seller_zip_code_prefix: integer (nullable = true)\n",
    "# |-- seller_city: string (nullable = true)\n",
    "# |-- seller_state: string (nullable = true)\n",
    "\n",
    "# |-- geolocation_zip_code_prefix: integer (nullable = true)\n",
    "# |-- lat: double (nullable = true)\n",
    "# |-- lng: double (nullable = true)\n",
    "\n",
    "# |-- order_id: string (nullable = true)\n",
    "# |-- order_purchase_timestamp: string (nullable = true)/home/jovyan/work/sales_analysis/\n",
    "\n",
    "\n",
    "\n",
    "# Calculating sum of money earned by sellers \n",
    "# and joining sellers table to be able to calculate \n",
    "# money earned partitioning by location \n",
    "sales = items\\\n",
    "    .groupBy('seller_id')\\\n",
    "    .agg({'price':'sum'})\\\n",
    "    .withColumnRenamed('sum(price)','revenue')\\\n",
    "    .join(sellers,['seller_id'])\n",
    "\n",
    "\n",
    "# Top 2 sellers in every state\n",
    "#using pyspark dataframe\n",
    "\n",
    "# Partition by state\n",
    "window_state = Window.partitionBy('seller_state').orderBy(col('revenue').desc())\n",
    "salesstate_df = sales\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_state','revenue')\\\n",
    "    .orderBy(col('revenue').desc())\n",
    "\n",
    "# Creating DataFrame containing seller_state, 1_best_seller, 2_best_seller\n",
    "window_seller_state = Window.partitionBy('seller_state').orderBy(col('revenue').desc())\n",
    "state_sellers = salesstate_df.withColumn('rank',rank().over(window_seller_state))\n",
    "best_sellers_state = state_sellers.filter(state_sellers.rank <= 2)\\\n",
    "   .withColumn('col', expr('concat(rank, \"_best_seller\")'))\\\n",
    "   .groupby('seller_state')\\\n",
    "   .pivot('col')\\\n",
    "   .agg(first(state_sellers.seller_id))\\\n",
    "   .na.fill(\"In this state is only one seller\", [\"2_best_seller\"])\n",
    "\n",
    "best_sellers_state.show()\n",
    "\n",
    "best_sellers_state.write.parquet(\"/home/jovyan/work/EPAM_sales_analysis/raport/transformed_data/4_task_city_best_sellers_state_df.parquet\",mode=\"overwrite\")\n",
    "\n",
    "# Top 2 sellers in every city\n",
    "# using pyspark dataframe\n",
    "\n",
    "# Partition by city\n",
    "window_state = Window.partitionBy('seller_city').orderBy(col('revenue').desc())\n",
    "salescity_df = sales\\\n",
    "    .withColumn('rank',rank().over(window_state))\\\n",
    "    .filter(col('rank')<=2)\\\n",
    "    .select('seller_id','seller_city','revenue')\\\n",
    "    .orderBy(col('revenue').desc())\n",
    "\n",
    "\n",
    "# Creating DataFrame containing seller_city, 1_best_seller, 2_best_seller\n",
    "window_seller = Window.partitionBy('seller_city').orderBy(col('revenue').desc())\n",
    "city_sellers = salescity_df.withColumn('rank',rank().over(window_seller))\n",
    "best_sellers = city_sellers.filter(city_sellers.rank <= 2)\\\n",
    "   .withColumn('col', expr('concat(rank, \"_best_seller\")'))\\\n",
    "   .groupby('seller_city')\\\n",
    "   .pivot('col')\\\n",
    "   .agg(first(city_sellers.seller_id))\\\n",
    "   .na.fill(\"In this city is only one seller\", [\"2_best_seller\"])\n",
    "\n",
    "best_sellers.show()\n",
    "\n",
    "best_sellers.write.parquet(\"/home/jovyan/work/EPAM_sales_analysis/raport/transformed_data/4_task_city_best_sellers_df.parquet\",mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "# Creating data frame with different locations sum of sales and number of sales \n",
    "salesmap_df = sales.join(geo,'zip_code','left').orderBy(col('revenue').desc())\n",
    "salescity_df = salesmap_df\\\n",
    "    .groupby('seller_city')\\\n",
    "    .agg({'revenue':'sum','lat':'avg','lng':'avg','seller_state':'count'})\\\n",
    "    .orderBy(col('sum(revenue)'))\\\n",
    "    .select('seller_city',\n",
    "            col('sum(revenue)').alias('revenue'),\n",
    "            col('count(seller_state)').alias('count'),\n",
    "            col('avg(lat)').alias('lat'),\n",
    "            col('avg(lng)').alias('lng'))\n",
    "\n",
    "\n",
    "salesstate_df = salesmap_df\\\n",
    "    .groupby('seller_state')\\\n",
    "    .agg({'revenue':'sum','lat':'avg','lng':'avg','seller_city':'count'})\\\n",
    "    .orderBy(col('sum(revenue)'))\\\n",
    "    .select('seller_state',\n",
    "            col('sum(revenue)').alias('revenue'),\n",
    "            col('count(seller_city)').alias('count'),\n",
    "            col('avg(lat)').alias('lat'),\n",
    "            col('avg(lng)').alias('lng'))\n",
    "\n",
    "\n",
    "# Function for further grouping of our results \n",
    "def count_group(x):\n",
    "    if x>1500:\n",
    "        return 0\n",
    "    elif x>200:\n",
    "        return 1\n",
    "    elif x>100:\n",
    "        return 2\n",
    "    elif x>20:\n",
    "        return 3\n",
    "    elif x>10:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "def sum_group(x):\n",
    "    if x>8000000:\n",
    "        return 0\n",
    "    elif x>1000000:\n",
    "        return 1\n",
    "    elif x>500000:\n",
    "        return 2\n",
    "    elif x>50000:\n",
    "        return 3\n",
    "    elif x>10000:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "\n",
    "colors = ['#CE4C18','#D58321','#D5B421','#C3E839','#43A85F','#66CF83']\n",
    "sizes = [25,12,8,5,3,1]\n",
    "\n",
    "\n",
    "rdd = salesstate_df.rdd.map(lambda row: (\n",
    "      row[\"seller_state\"],row[\"revenue\"],row[\"count\"],row[\"lat\"],row[\"lng\"], sizes[count_group(row[\"count\"])], colors[count_group(row[\"count\"])])\n",
    "  )\n",
    "state_df = rdd.toDF([\"seller_state\",\"revenue\",\"count\",\"lat\", \"lng\", \"size\", \"colors\"])\n",
    "\n",
    "\n",
    "rdd_2 = salescity_df.rdd.map(lambda row: (\n",
    "      row[\"seller_city\"],row[\"revenue\"],row[\"count\"],row[\"lat\"],row[\"lng\"], sizes[count_group(row[\"count\"])], colors[count_group(row[\"count\"])])\n",
    "  )\n",
    "city_df = rdd_2.toDF([\"seller_city\",\"revenue\",\"count\",\"lat\", \"lng\", \"size\", \"colors\"])\n",
    "\n",
    "# Saving files that will be used when creating report\n",
    "state_df.write.parquet(\"/home/jovyan/work/EPAM_sales_analysis/raport/transformed_data/6_task_salesstate_df.parquet\" \\\n",
    "                            ,mode=\"overwrite\")\n",
    "city_df.write.parquet(\"/home/jovyan/work/EPAM_sales_analysis/raport/transformed_data/4_task_salescity_df.parquet\" \\\n",
    "                           ,mode=\"overwrite\")\n",
    "\n",
    "state_df.show()\n",
    "city_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
